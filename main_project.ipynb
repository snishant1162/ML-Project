{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ac852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52eeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'photos/train'\n",
    "val_dir = 'photos/test'\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,    \n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    "    vertical_flip=False, # Don't randomly flip images vertically\n",
    "    )\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd47618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=16,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd1b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=16,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470f7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d5aa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f2a146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c9ae60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cda4802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 22, 22, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,345,607\n",
      "Trainable params: 2,345,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emotion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94eaff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e4c0dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 1.7773 - accuracy: 0.2730 - val_loss: 1.6747 - val_accuracy: 0.3291\n",
      "Epoch 2/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 1.6488 - accuracy: 0.3498 - val_loss: 1.5195 - val_accuracy: 0.4176\n",
      "Epoch 3/50\n",
      "1794/1794 [==============================] - 153s 86ms/step - loss: 1.5492 - accuracy: 0.4002 - val_loss: 1.4048 - val_accuracy: 0.4618\n",
      "Epoch 4/50\n",
      "1794/1794 [==============================] - 146s 81ms/step - loss: 1.4690 - accuracy: 0.4346 - val_loss: 1.3250 - val_accuracy: 0.4958\n",
      "Epoch 5/50\n",
      "1794/1794 [==============================] - 146s 82ms/step - loss: 1.4005 - accuracy: 0.4654 - val_loss: 1.2709 - val_accuracy: 0.5119\n",
      "Epoch 6/50\n",
      "1794/1794 [==============================] - 149s 83ms/step - loss: 1.3557 - accuracy: 0.4870 - val_loss: 1.2189 - val_accuracy: 0.5333\n",
      "Epoch 7/50\n",
      "1794/1794 [==============================] - 148s 83ms/step - loss: 1.3117 - accuracy: 0.5009 - val_loss: 1.1991 - val_accuracy: 0.5495\n",
      "Epoch 8/50\n",
      "1794/1794 [==============================] - 148s 82ms/step - loss: 1.2838 - accuracy: 0.5134 - val_loss: 1.1666 - val_accuracy: 0.5573\n",
      "Epoch 9/50\n",
      "1794/1794 [==============================] - 146s 81ms/step - loss: 1.2595 - accuracy: 0.5203 - val_loss: 1.1610 - val_accuracy: 0.5642\n",
      "Epoch 10/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 1.2355 - accuracy: 0.5338 - val_loss: 1.1348 - val_accuracy: 0.5696\n",
      "Epoch 11/50\n",
      "1794/1794 [==============================] - 144s 81ms/step - loss: 1.2177 - accuracy: 0.5373 - val_loss: 1.1242 - val_accuracy: 0.5735\n",
      "Epoch 12/50\n",
      "1794/1794 [==============================] - 144s 81ms/step - loss: 1.2031 - accuracy: 0.5453 - val_loss: 1.1131 - val_accuracy: 0.5788\n",
      "Epoch 13/50\n",
      "1794/1794 [==============================] - 144s 80ms/step - loss: 1.1895 - accuracy: 0.5508 - val_loss: 1.0913 - val_accuracy: 0.5873\n",
      "Epoch 14/50\n",
      "1794/1794 [==============================] - 144s 81ms/step - loss: 1.1721 - accuracy: 0.5564 - val_loss: 1.0897 - val_accuracy: 0.5872\n",
      "Epoch 15/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 1.1628 - accuracy: 0.5591 - val_loss: 1.0892 - val_accuracy: 0.5887\n",
      "Epoch 16/50\n",
      "1794/1794 [==============================] - 142s 79ms/step - loss: 1.1481 - accuracy: 0.5647 - val_loss: 1.0748 - val_accuracy: 0.5965\n",
      "Epoch 17/50\n",
      "1794/1794 [==============================] - 143s 80ms/step - loss: 1.1397 - accuracy: 0.5692 - val_loss: 1.0570 - val_accuracy: 0.6006\n",
      "Epoch 18/50\n",
      "1794/1794 [==============================] - 144s 80ms/step - loss: 1.1282 - accuracy: 0.5700 - val_loss: 1.0521 - val_accuracy: 0.6004\n",
      "Epoch 19/50\n",
      "1794/1794 [==============================] - 144s 81ms/step - loss: 1.1238 - accuracy: 0.5741 - val_loss: 1.0524 - val_accuracy: 0.6035\n",
      "Epoch 20/50\n",
      "1794/1794 [==============================] - 145s 81ms/step - loss: 1.1105 - accuracy: 0.5800 - val_loss: 1.0656 - val_accuracy: 0.5936\n",
      "Epoch 21/50\n",
      "1794/1794 [==============================] - 145s 81ms/step - loss: 1.1028 - accuracy: 0.5843 - val_loss: 1.0299 - val_accuracy: 0.6120\n",
      "Epoch 22/50\n",
      "1794/1794 [==============================] - 143s 80ms/step - loss: 1.0943 - accuracy: 0.5856 - val_loss: 1.0357 - val_accuracy: 0.6099\n",
      "Epoch 23/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 1.0818 - accuracy: 0.5897 - val_loss: 1.0253 - val_accuracy: 0.6120\n",
      "Epoch 24/50\n",
      "1794/1794 [==============================] - 145s 81ms/step - loss: 1.0777 - accuracy: 0.5910 - val_loss: 1.0177 - val_accuracy: 0.6131\n",
      "Epoch 25/50\n",
      "1794/1794 [==============================] - 144s 80ms/step - loss: 1.0715 - accuracy: 0.5962 - val_loss: 1.0123 - val_accuracy: 0.6177\n",
      "Epoch 26/50\n",
      "1794/1794 [==============================] - 144s 80ms/step - loss: 1.0657 - accuracy: 0.5974 - val_loss: 1.0054 - val_accuracy: 0.6225\n",
      "Epoch 27/50\n",
      "1794/1794 [==============================] - 144s 80ms/step - loss: 1.0556 - accuracy: 0.5999 - val_loss: 1.0003 - val_accuracy: 0.6232\n",
      "Epoch 28/50\n",
      "1794/1794 [==============================] - 143s 80ms/step - loss: 1.0553 - accuracy: 0.6023 - val_loss: 0.9992 - val_accuracy: 0.6246\n",
      "Epoch 29/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 1.0502 - accuracy: 0.6064 - val_loss: 1.0133 - val_accuracy: 0.6201\n",
      "Epoch 30/50\n",
      "1794/1794 [==============================] - 148s 83ms/step - loss: 1.0385 - accuracy: 0.6067 - val_loss: 0.9914 - val_accuracy: 0.6275\n",
      "Epoch 31/50\n",
      "1794/1794 [==============================] - 148s 83ms/step - loss: 1.0338 - accuracy: 0.6104 - val_loss: 0.9944 - val_accuracy: 0.6261\n",
      "Epoch 32/50\n",
      "1794/1794 [==============================] - 148s 83ms/step - loss: 1.0286 - accuracy: 0.6124 - val_loss: 1.0003 - val_accuracy: 0.6270\n",
      "Epoch 33/50\n",
      "1794/1794 [==============================] - 149s 83ms/step - loss: 1.0224 - accuracy: 0.6129 - val_loss: 0.9800 - val_accuracy: 0.6341\n",
      "Epoch 34/50\n",
      "1794/1794 [==============================] - 148s 83ms/step - loss: 1.0187 - accuracy: 0.6179 - val_loss: 0.9872 - val_accuracy: 0.6288\n",
      "Epoch 35/50\n",
      "1794/1794 [==============================] - 149s 83ms/step - loss: 1.0075 - accuracy: 0.6208 - val_loss: 0.9686 - val_accuracy: 0.6422\n",
      "Epoch 36/50\n",
      "1794/1794 [==============================] - 148s 82ms/step - loss: 1.0073 - accuracy: 0.6199 - val_loss: 0.9746 - val_accuracy: 0.6330\n",
      "Epoch 37/50\n",
      "1794/1794 [==============================] - 148s 82ms/step - loss: 1.0016 - accuracy: 0.6237 - val_loss: 0.9772 - val_accuracy: 0.6335\n",
      "Epoch 38/50\n",
      "1794/1794 [==============================] - 148s 83ms/step - loss: 0.9989 - accuracy: 0.6226 - val_loss: 0.9721 - val_accuracy: 0.6391\n",
      "Epoch 39/50\n",
      "1794/1794 [==============================] - 152s 85ms/step - loss: 0.9915 - accuracy: 0.6296 - val_loss: 0.9606 - val_accuracy: 0.6437\n",
      "Epoch 40/50\n",
      "1794/1794 [==============================] - 154s 86ms/step - loss: 0.9884 - accuracy: 0.6292 - val_loss: 0.9658 - val_accuracy: 0.6384\n",
      "Epoch 41/50\n",
      "1794/1794 [==============================] - 142s 79ms/step - loss: 0.9877 - accuracy: 0.6321 - val_loss: 0.9595 - val_accuracy: 0.6447\n",
      "Epoch 42/50\n",
      "1794/1794 [==============================] - 141s 78ms/step - loss: 0.9812 - accuracy: 0.6332 - val_loss: 0.9689 - val_accuracy: 0.6452\n",
      "Epoch 43/50\n",
      "1794/1794 [==============================] - 143s 80ms/step - loss: 0.9766 - accuracy: 0.6315 - val_loss: 0.9601 - val_accuracy: 0.6415\n",
      "Epoch 44/50\n",
      "1794/1794 [==============================] - 151s 84ms/step - loss: 0.9699 - accuracy: 0.6334 - val_loss: 0.9573 - val_accuracy: 0.6426\n",
      "Epoch 45/50\n",
      "1794/1794 [==============================] - 149s 83ms/step - loss: 0.9701 - accuracy: 0.6336 - val_loss: 0.9533 - val_accuracy: 0.6458\n",
      "Epoch 46/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 0.9601 - accuracy: 0.6377 - val_loss: 0.9676 - val_accuracy: 0.6402\n",
      "Epoch 47/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 0.9594 - accuracy: 0.6378 - val_loss: 0.9435 - val_accuracy: 0.6551\n",
      "Epoch 48/50\n",
      "1794/1794 [==============================] - 145s 81ms/step - loss: 0.9549 - accuracy: 0.6413 - val_loss: 0.9559 - val_accuracy: 0.6430\n",
      "Epoch 49/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 0.9515 - accuracy: 0.6416 - val_loss: 0.9484 - val_accuracy: 0.6454\n",
      "Epoch 50/50\n",
      "1794/1794 [==============================] - 147s 82ms/step - loss: 0.9524 - accuracy: 0.6438 - val_loss: 0.9496 - val_accuracy: 0.6475\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = emotion_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 16,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2937a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model_5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1aac753",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('emotion_model_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02349cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('emotion_model_5.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"emotion_model_5.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "########################################\n",
    "size = 300\n",
    "## Angry\n",
    "angry_img = cv2.imread('./emojis/angry.png')\n",
    "angry_img = cv2.resize(angry_img, (size, size))\n",
    "\n",
    "angry_img2gray = cv2.cvtColor(angry_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, angry = cv2.threshold(angry_img2gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "## Disgusted\n",
    "disgusted_img = cv2.imread('./emojis/disgusted.jfif')\n",
    "disgusted_img = cv2.resize(disgusted_img, (size, size))\n",
    "\n",
    "disgusted_img2gray = cv2.cvtColor(disgusted_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, disgusted = cv2.threshold(disgusted_img2gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "## Fearfull\n",
    "fearful_img = cv2.imread('./emojis/fearful.png')\n",
    "fearful_img = cv2.resize(fearful_img, (size, size))\n",
    "\n",
    "fearful_img2gray = cv2.cvtColor(fearful_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, fearful = cv2.threshold(fearful_img2gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "## Happy\n",
    "happy_img = cv2.imread('./emojis/happy.png')\n",
    "happy_img = cv2.resize(happy_img, (size, size))\n",
    "\n",
    "happy_img2gray = cv2.cvtColor(happy_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, happy = cv2.threshold(happy_img2gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "## neutral\n",
    "neutral_img = cv2.imread('./emojis/neutral.png')\n",
    "neutral_img = cv2.resize(neutral_img, (size, size))\n",
    "\n",
    "neutral_img2gray = cv2.cvtColor(neutral_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, neutral = cv2.threshold(neutral_img2gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "## sad\n",
    "sad_img = cv2.imread('./emojis/sad.png')\n",
    "sad_img = cv2.resize(sad_img, (size, size))\n",
    "\n",
    "sad_img2gray = cv2.cvtColor(sad_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, sad = cv2.threshold(sad_img2gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "## surprised\n",
    "surprised_img = cv2.imread('./emojis/surprised.png')\n",
    "surprised_img = cv2.resize(surprised_img, (size, size))\n",
    "\n",
    "surprised_img2gray = cv2.cvtColor(surprised_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, surprised = cv2.threshold(surprised_img2gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "########################################\n",
    "\n",
    "emotion_mask_dist = {0:angry,1:disgusted,2:fearful,3:happy,4:neutral,5:sad,6:surprised}\n",
    "emotion_dist = {0:angry_img,1:disgusted_img,2:fearful_img,3:happy_img,4:neutral_img,5:sad_img,6:surprised_img}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier(r'haarcascades\\haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces available on camera\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # take each face available on the camera and Preprocess it\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (206, 235, 135), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        \n",
    "        roi = frame[-size-10:-10, -size-10:-10]\n",
    "        roi[np.where(emotion_mask_dist[maxindex])] = 0\n",
    "        roi += emotion_dist[maxindex]\n",
    "        \n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904d63a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
